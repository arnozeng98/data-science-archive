{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLX 535 Lab Assignment 2: Working with CFG Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will \n",
    "- Use the Stanford CoreNLP parser to parse new text into constituency trees\n",
    "- Create a parsing gold standard and use it to evaluate parsers\n",
    "- Build a context-free grammar from existing parses (optional assignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does this relate to class?\n",
    "\n",
    "In class, we're discussing CFGs (Context-free grammars).  In this lab, you will be evaluating the quality of the Stanford Parser, and then using it to create and modify your own CFG (and feature grammar), to establish a treebank of your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment requires that you have set up the Stanford parser. First, make sure you have the more recent version of [Java](https://www.java.com/en/download/), then get the [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/) package. Make sure that you get the 4.3.2 version (or at least >=4.2.0).  (It's probably easiest to download from [HuggingFace]( https://huggingface.co/stanfordnlp/CoreNLP/tree/main))\n",
    "\n",
    "To load Stanford CoreNLP in Python, change the `coreNLP_dir` variable in the code below to where you unzipped Stanford coreNLP. You can follow this [tutorial](https://bbengfort.github.io/snippets/2018/06/22/corenlp-nltk-parses.html). Once the coreNLP server is running, you will be able to access it through NLTK.\n",
    "\n",
    "There are 3 steps:\n",
    "\n",
    "* Install Java (the JRE is fine for this assignment, but you may want to consider the JDK if you want to do Java development)\n",
    "* Set an environment variable called JAVA_HOME to the location of your java.exe file (you might need to restart your machine after setting this variable)\n",
    "* Download the coreNLP, unzip it, and point to it in the code below.\n",
    "\n",
    "It may take a few seconds or up to a minute to start the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.corenlp import CoreNLPServer\n",
    "import os\n",
    "import time\n",
    "\n",
    "coreNLP_dir = os.path.join('models', 'stanford-corenlp-4.5.8')\n",
    "\n",
    "# # Create the server\n",
    "# server = CoreNLPServer(\n",
    "#    os.path.join(coreNLP_dir, \"stanford-corenlp-4.5.8.jar\"),\n",
    "#    os.path.join(coreNLP_dir, \"stanford-corenlp-4.5.8-models.jar\")    \n",
    "# )\n",
    "\n",
    "# # Start the server in the background\n",
    "# server.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models\\\\stanford-corenlp-4.5.8'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coreNLP_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run following in terminal:\n",
    "\n",
    "```ba\n",
    "java -mx4g -cp 'models\\\\stanford-corenlp-4.5.8/*' edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to parse a sentence to make sure that everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (S\n",
      "    (NP (PRP I))\n",
      "    (VP\n",
      "      (VBD put)\n",
      "      (NP (DT the) (NN book))\n",
      "      (PP (IN in) (NP (DT the) (NN box)))\n",
      "      (PP (IN on) (NP (DT the) (NN table))))\n",
      "    (. .)))\n"
     ]
    }
   ],
   "source": [
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "\n",
    "parser = CoreNLPParser()\n",
    "parse = next(parser.raw_parse(\"I put the book in the box on the table.\"))\n",
    "print(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         ROOT                              \n",
      "                          |                                 \n",
      "                          S                                \n",
      "  ________________________|______________________________   \n",
      " |                        VP                             | \n",
      " |    ____________________|________________              |  \n",
      " |   |       |            PP               PP            | \n",
      " |   |       |         ___|____         ___|___          |  \n",
      " NP  |       NP       |        NP      |       NP        | \n",
      " |   |    ___|___     |    ____|___    |    ___|____     |  \n",
      "PRP VBD  DT      NN   IN  DT       NN  IN  DT       NN   . \n",
      " |   |   |       |    |   |        |   |   |        |    |  \n",
      " I  put the     book  in the      box  on the     table  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "list(parser.raw_parse('I put the book in the box on the table.'))[0].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (S\n",
      "    (NP (PRP They))\n",
      "    (VP\n",
      "      (VBD gave)\n",
      "      (NP (PRP me))\n",
      "      (NP (ADJP (JJ yellow) (CC and) (JJ blue)) (NNS pants)))))\n"
     ]
    }
   ],
   "source": [
    "parse = next(parser.raw_parse(\"They gave me yellow and blue pants\"))\n",
    "print(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      S                                 \n",
      "  ____|_________                         \n",
      " |              VP                      \n",
      " |     _________|__________              \n",
      " |    |    |               NP           \n",
      " |    |    |           ____|_________    \n",
      " NP   |    NP        ADJP            |  \n",
      " |    |    |     _____|________      |   \n",
      "PRP  VBD  PRP   JJ    CC       JJ   NNS \n",
      " |    |    |    |     |        |     |   \n",
      "They gave  me yellow and      blue pants\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list(next(parser.raw_parse(\"They gave me yellow and blue pants\")))[0].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below if you want to shut down the coreNLP server after you've finished with it. It's a good idea to shut down the parser after finishing work with it because it may remain running in the background and you may not be able to start another parser instance without restarting your computer or manually killing the parser process. \n",
    "\n",
    "If you forget to stop the server, next time when you try to launch it, you'll get an error. In this case, you may first need to kill the old server manually. To do this, you can run `ps -ax | grep stanford` on the commandline (at least on OSX and Linux) which should give you the process ID of the server, e.g. 11111. You can then use `kill -9 11111` to kill the parser, after which you should be able to start the server again.  On Windows, you should be able to use the Task Manager to kill the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    server.stop()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other things you'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\ArnoZ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tree import Tree\n",
    "from nltk.grammar import CFG, Nonterminal, Production, FeatureGrammar\n",
    "from nltk.parse import CoreNLPParser\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy Submission\n",
    "\n",
    "rubric={mechanics:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the marks for tidy submission:\n",
    "\n",
    "- Submit the assignment by filling in this jupyter notebook with your answers embedded\n",
    "- Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Test the parser\n",
    "\n",
    "#### 1.1\n",
    "rubric={accuracy:2}\n",
    "\n",
    "Get the Stanford parser working, then parse the first 20 sentences of the NLTK `movie_reviews` corpus, and report the average depth (height) of the parse trees. If you find the parser is failing to parse something, you can skip over it.\n",
    "\n",
    "You should retain the tokenization in the `movie_reviews` corpus. You can use `parser.parse()` to parse the tokenized input sentences. Note that `parser.parse()` returns an iterator over possible parses. There may be several if the sentence is ambiguous. You can compute statistics on the first sentence which is returned by `parser.parse()` (ie, don't calculate an average height over all parses - we're just interested in the height of the first parse, for simplicity.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Height 1: 10\n",
      "Height 2: 7\n",
      "Height 3: 12\n",
      "Height 4: 7\n",
      "Height 5: 8\n",
      "Height 6: 4\n",
      "Height 7: 4\n",
      "Height 8: 12\n",
      "Height 9: 20\n",
      "Height 10: 12\n",
      "Height 11: 8\n",
      "Height 12: 9\n",
      "Height 13: 17\n",
      "Height 14: 12\n",
      "Height 15: 15\n",
      "Height 16: 15\n",
      "Height 17: 10\n",
      "Height 18: 5\n",
      "Height 19: 24\n",
      "Height 20: 23\n",
      "\n",
      "Average height: 11.7\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# your code here\n",
    "\n",
    "movie_reviews_sentences = list(movie_reviews.sents())[:20]\n",
    "heights = []\n",
    "\n",
    "for i, sentence in enumerate(movie_reviews_sentences):\n",
    "    \n",
    "    try:\n",
    "        parse_tree = next(parser.raw_parse(\" \".join(sentence)))\n",
    "        h = parse_tree.height()\n",
    "        print(f\"Height {i + 1}: {h}\")\n",
    "        heights.append(h)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse: {' '.join(sentence)}\\nError: {e}\")\n",
    "\n",
    "average_height = sum(heights) / len(heights)\n",
    "print(f\"\\nAverage height: {average_height}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Build a gold standard\n",
    "\n",
    "One typical way to build treebanks is, rather than having humans build a tree from scratch, instead use an automatic parser to give an initial parse, and then have humans do a second pass to fix any errors. That's what you're going to do in this exercise. \n",
    "\n",
    "We will use the following three sentences from the NLTK `movie_reviews` corpus to build a mini-treebank:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    ['oh', ',', 'and', 'by', 'the', 'way', ',', 'this', 'is', \n",
    "     'not', 'a', 'horror', 'or', 'teen', 'slasher', 'flick', '.'],\n",
    "    \n",
    "    ['little', 'do', 'they', 'know', 'the', 'power', 'within', '.'],\n",
    "    \n",
    "    ['so', ',', 'if', 'robots', 'and', 'body', 'parts', 'really', \n",
    "     'turn', 'you', 'on', ',', 'here', \"'\", 's', 'your', 'movie', '.']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each of the three sections below you will see a parse tree produced by CoreNLPParser. Each of the trees contains at least one parse error. You'll see a brief informal description of the error and it is your task to fix the tree.  \n",
    "\n",
    "Create an NLTK Tree corresponding to the correct parse, which should be appended to the `gold_standard_parses` list below. You can do this manually by printing the tree, creating a triple-quoted string, modifying it, and converting it back into a `Tree` using the function `Tree.fromstring` following the example below. If you are unsure exactly how to correct something, read through the lecture slides. Many common parse errors are explained there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      S      \n",
      "  ____|____   \n",
      " NP   VP   | \n",
      " |    |    |  \n",
      "NNS  VBN   . \n",
      " |    |    |  \n",
      "Dogs bark  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "\n",
    "# The second phrase should be a VP, not an NP\n",
    "err_tree_str = '''(S\n",
    "(NP (NNS Dogs)) \n",
    "(NP (VBN bark))\n",
    "(. .))\n",
    "'''\n",
    "\n",
    "corr_tree_str = '''(S\n",
    "(NP (NNS Dogs)) \n",
    "(VP (VBN bark))\n",
    "(. .))\n",
    "'''\n",
    "\n",
    "corr_tree = Tree.fromstring(corr_tree_str)\n",
    "corr_tree.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store your corrected nltk.Tree objects in this list\n",
    "gold_standard_parses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence 1\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      ROOT                                                  \n",
      "                                       |                                                     \n",
      "                                       S                                                    \n",
      "  _____________________________________|__________________________________________________   \n",
      " |    |   |       |           |   |                      VP                               | \n",
      " |    |   |       |           |   |     _________________|_________                       |  \n",
      " |    |   |       |           |   |    |    |                      NP                     | \n",
      " |    |   |       |           |   |    |    |                   ___|_________________     |  \n",
      " |    |   |       PP          |   |    |    |                 NML                    |    | \n",
      " |    |   |    ___|___        |   |    |    |        __________|________             |    |  \n",
      "INTJ  |   |   |       NP      |   NP   |    |       NP         |        NP           |    | \n",
      " |    |   |   |    ___|___    |   |    |    |    ___|____      |    ____|_____       |    |  \n",
      " UH   ,   CC  IN  DT      NN  ,   DT  VBZ   RB  DT       NN    CC  NN         NN     NN   . \n",
      " |    |   |   |   |       |   |   |    |    |   |        |     |   |          |      |    |  \n",
      " oh   ,  and  by the     way  ,  this  is  not  a      horror  or teen     slasher flick  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The word \"flick\" should be modified by the entire noun phrase \n",
    "# \"a horror or teen slasher\" instead of just \"teen slasher\". A noun \n",
    "# phrase which modifies a noun is labeled as NML.\n",
    "\n",
    "err_tree_str = '''(ROOT\n",
    "  (S\n",
    "    (INTJ (UH oh))\n",
    "    (, ,)\n",
    "    (CC and)\n",
    "    (PP (IN by) (NP (DT the) (NN way)))\n",
    "    (, ,)\n",
    "    (NP (DT this))\n",
    "    (VP\n",
    "      (VBZ is)\n",
    "      (RB not)\n",
    "      (NP\n",
    "        (NP (DT a) (NN horror))\n",
    "        (CC or)\n",
    "        (NP (NML (NN teen) (NN slasher)) (NN flick))))\n",
    "    (. .)))'''\n",
    "\n",
    "# your code here\n",
    "\n",
    "corr_tree_str = '''(ROOT\n",
    "  (S\n",
    "    (INTJ (UH oh))\n",
    "    (, ,)\n",
    "    (CC and)\n",
    "    (PP (IN by) (NP (DT the) (NN way)))\n",
    "    (, ,)\n",
    "    (NP (DT this))\n",
    "    (VP\n",
    "      (VBZ is)\n",
    "      (RB not)\n",
    "      (NP\n",
    "        (NML\n",
    "          (NP (DT a) (NN horror))\n",
    "          (CC or)\n",
    "          (NP (NN teen) (NN slasher)))\n",
    "        (NN flick)))\n",
    "    (. .)))'''\n",
    "\n",
    "corr_tree = Tree.fromstring(corr_tree_str)\n",
    "gold_standard_parses.append(corr_tree)\n",
    "corr_tree.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence 2\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                ROOT                          \n",
      "                 |                             \n",
      "                 S                            \n",
      "   ______________|__________________________   \n",
      "  |              VP                         | \n",
      "  |      ________|____                      |  \n",
      "  |     |            SBAR                   | \n",
      "  |     |             |                     |  \n",
      "  |     |             S                     | \n",
      "  |     |    _________|____                 |  \n",
      "  |     |   |              VP               | \n",
      "  |     |   |     _________|____            |  \n",
      "  |     |   |    |              NP          | \n",
      "  |     |   |    |          ____|_____      |  \n",
      "  NP    |   NP   |         NP         PP    | \n",
      "  |     |   |    |     ____|____      |     |  \n",
      "  RB   VBP PRP  VBP   DT        NN    IN    . \n",
      "  |     |   |    |    |         |     |     |  \n",
      "little  do they know the      power within  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The PP \"within\" should attach to the NP \"the power\", not to the VP \"know the power\". \n",
    "\n",
    "err_tree_str = '''(ROOT\n",
    "  (S\n",
    "    (NP (RB little))\n",
    "    (VP\n",
    "      (VBP do)\n",
    "      (SBAR\n",
    "        (S\n",
    "          (NP (PRP they))\n",
    "          (VP (VBP know) (NP (DT the) (NN power)) (PP (IN within))))))\n",
    "    (. .)))'''\n",
    "\n",
    "# your code here\n",
    "\n",
    "corr_tree_str = '''(ROOT\n",
    "  (S\n",
    "    (NP (RB little))\n",
    "    (VP\n",
    "      (VBP do)\n",
    "      (SBAR\n",
    "        (S\n",
    "          (NP (PRP they))\n",
    "          (VP\n",
    "            (VBP know)\n",
    "            (NP\n",
    "              (NP (DT the) (NN power))\n",
    "              (PP (IN within)))))))\n",
    "    (. .)))'''\n",
    "\n",
    "corr_tree = Tree.fromstring(corr_tree_str)\n",
    "gold_standard_parses.append(corr_tree)\n",
    "corr_tree.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence 3\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          ROOT                                            \n",
      "                                           |                                               \n",
      "                                          SINV                                            \n",
      "  _________________________________________|____________________________________________   \n",
      " |    |        |                     |          VP                                      | \n",
      " |    |        |                     |      ____|___                                    |  \n",
      " |    |        |                     |     |        NP                                  | \n",
      " |    |        |                     |     |     ___|_______                            |  \n",
      " |    |        |                     |     |    |           PP                          | \n",
      " |    |        |                     |     |    |    _______|________                   |  \n",
      " |    |        PP                    |     |    |   |   |            S                  | \n",
      " |    |    ____|_____                |     |    |   |   |    ________|___               |  \n",
      " |    |   |          NP              |     |    |   |   |   |            VP             | \n",
      " |    |   |          |               |     |    |   |   |   |     _______|____          |  \n",
      "ADVP  |   |         NML             ADVP   |    NP  |   |   NP   |            NP        | \n",
      " |    |   |     _____|_________      |     |    |   |   |   |    |        ____|____     |  \n",
      " RB   ,   IN  NNS    CC  NN   NNS    RB   VBP  PRP  IN  ,   EX  VBZ     PRP$       NN   . \n",
      " |    |   |    |     |   |     |     |     |    |   |   |   |    |       |         |    |  \n",
      " so   ,   if robots and body parts really turn you  on  ,  here  's     your     movie  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# There are several errors.\"body\" and \"parts\" should form an NP. \n",
    "# Moreover, \"here's your movie\" should form a clause and \"s\" is in \n",
    "# fact a verb.  \n",
    "\n",
    "err_tree_str = '''(ROOT\n",
    "  (SINV\n",
    "    (ADVP (RB so))\n",
    "    (, ,)\n",
    "    (PP\n",
    "      (IN if)\n",
    "      (NP (NML (NNS robots) (CC and) (NN body)) (NNS parts)))\n",
    "    (ADVP (RB really))\n",
    "    (VP\n",
    "      (VBP turn)\n",
    "      (NP (NP (PRP you)) (PP (IN on) (, ,) (NP (RB here))) ('' '))\n",
    "      (S (NP (POS s))))\n",
    "    (NP (PRP$ your) (NN movie))\n",
    "    (. .)))'''\n",
    "\n",
    "# your code here\n",
    "\n",
    "corr_tree_str = '''(ROOT\n",
    "  (SINV\n",
    "    (ADVP (RB so))\n",
    "    (, ,)\n",
    "    (PP\n",
    "      (IN if)\n",
    "      (NP (NML (NNS robots) (CC and) (NN body) (NNS parts))))\n",
    "    (ADVP (RB really))\n",
    "    (VP\n",
    "      (VBP turn)\n",
    "      (NP\n",
    "        (NP (PRP you))\n",
    "        (PP\n",
    "          (IN on)\n",
    "          (, ,)\n",
    "          (S\n",
    "            (NP (EX here))\n",
    "            (VP (VBZ 's)\n",
    "                (NP (PRP$ your) (NN movie))))))\n",
    "      )\n",
    "    (. .)))'''\n",
    "\n",
    "corr_tree = Tree.fromstring(corr_tree_str)\n",
    "gold_standard_parses.append(corr_tree)\n",
    "corr_tree.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Evaluating parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a gold standard, we can use it to evaluate parser output. \n",
    "\n",
    "#### 3.1\n",
    "rubric={accuracy:3,quality:1}\n",
    "\n",
    "Start by writing a function, get_constituents, which takes a parse tree and returns a set of tuples, where each tuple is (*label*, *start*, *end*) where *start* and *end* correspond to the indices of a corresponding constituent (phrase) in the sentence and *label* is the label of that constituent. \n",
    "\n",
    "Do **not** include simple POS constituents `(POS word)` like `(VBD ate)`. We want to evaluate the parser only on actual phrases.\n",
    "\n",
    "**HINT:** You may want to use recursion to solve this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_constituents(tree: Tree, start_index: int=0) -> set:\n",
    "    constituents = set()\n",
    "\n",
    "    # your code here\n",
    "    \n",
    "    def traverse_with_root(node: Tree, start_index: int):\n",
    "        # Add the current node to constituents \n",
    "        # if it has children or it is the final single-branch node\n",
    "        if len(node) > 1 or (len(node) == 1 and isinstance(node[0], Tree)):\n",
    "            end_index = start_index + len(node.leaves())\n",
    "            constituents.add((node.label(), start_index, end_index))\n",
    "\n",
    "        # Recursively traverse children\n",
    "        current_index = start_index\n",
    "        for child in node:\n",
    "            if isinstance(child, Tree):\n",
    "                traverse_with_root(child, current_index) # recursion\n",
    "                current_index += len(child.leaves())\n",
    "            else:\n",
    "                current_index += 1\n",
    "\n",
    "    traverse_with_root(tree, start_index)\n",
    "    return constituents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "tree = Tree.fromstring('''(S (NP (DT the) (DT mouse)) (VP (VBD ate) (NP (NP (DT the) (DT mouse)) (POS 's) (NN cheese))) )''')\n",
    "assert get_constituents(tree) == {(\"S\",0,7), (\"NP\",0,2), (\"VP\",2,7),(\"NP\",3,7),(\"NP\",3,5)}\n",
    "tree = Tree.fromstring('''(S (NP (DET the) (NP (NN cat) (CC and) (NN dog))) (VP (VBD fought)))''')\n",
    "assert get_constituents(tree) == {(\"S\",0,5), (\"NP\",0,4), (\"NP\",1,4),(\"VP\",4,5)}\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2\n",
    "rubric={accuracy:2,efficiency:1}\n",
    "\n",
    "Write a function `parse_f1` which uses get_constituents to implement the constituent F-score measure discussed in the lecture and reading. It should be given two lists, a lists of proposed parses and a corresponding list of gold standard parses, and return an F-score reflecting how close the proposed parses match. For full points, you should keep a running count of the relevant numbers over the entire set, and not average f-score across the individual sentences. \n",
    "\n",
    "**Hint:** to get the efficiency point, you should take advantage of Python's fast set operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_f1(proposed_parses: list, gold_parses: list) -> float:\n",
    "    f1score = 0\n",
    "    \n",
    "    # your code here\n",
    "\n",
    "    # Initialize counts for precision and recall\n",
    "    true_positives = 0\n",
    "    total_proposed = 0\n",
    "    total_gold = 0\n",
    "\n",
    "    # Loop through the parses\n",
    "    for proposed_tree, gold_tree in zip(proposed_parses, gold_parses):\n",
    "        proposed_constituents = get_constituents(proposed_tree)\n",
    "        gold_constituents = get_constituents(gold_tree)\n",
    "\n",
    "        # Count overlaps (true positives)\n",
    "        true_positives += len(proposed_constituents & gold_constituents)\n",
    "        # Count total proposed and gold constituents\n",
    "        total_proposed += len(proposed_constituents)\n",
    "        total_gold += len(gold_constituents)\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = true_positives / total_proposed if total_proposed > 0 else 0\n",
    "    recall = true_positives / total_gold if total_gold > 0 else 0\n",
    "    f1score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "gold_parses = [Tree.fromstring('''(S (NP (DT the) (DT mouse)) (VP (VBD ate) (NP (NP (DT the) (DT mouse)) (POS 's) (NN cheese))) )'''), Tree.fromstring('''(S (NP (NNS mice)) (VP (VBD love) (NP (NNS ducks))))''')]\n",
    "proposed_parses = [Tree.fromstring('''(S (NP (DT the) (DT mouse)) (VP (VBD ate) (NP (NP (DT the) (DT mouse)) (POS 's) (NN cheese))) )'''), Tree.fromstring('''(S (NP (NNS mice) (NN love)) (VP (VBZ ducks)))''')]\n",
    "assert 0.71 > parse_f1(proposed_parses, gold_parses) > 0.7\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3\n",
    "\n",
    "rubric={accuracy:1}\n",
    "\n",
    "Parse your three example sentences from assignment 2 using CoreNLPParser (you can find the sentences in the list `corpus`), extract the constituents and evaluate the result against `gold_standard_parses`. \n",
    "\n",
    "**Hint:** Your F1-score should be > 0.6 (the actual score may depend a bit on your version of the CoreNLP parser)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.684931506849315"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "# Parse sentences\n",
    "proposed_parses = [next(parser.raw_parse(\" \".join(sentence))) for sentence in corpus]\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = parse_f1(proposed_parses, gold_standard_parses)\n",
    "f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4\n",
    "\n",
    "rubric={reasoning:1}\n",
    "\n",
    "Given the way we build our gold standard, do you think this is a valid indication of the quality of parsers? Why or why not? What about if we tested the parser on the Penn Treebank corpus instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "I don't think it is valid due to these problems. Firstly, there are only three sentences, this insufficient in diversity of syntactic structures may cause subjectivity and bias. And the robustness and generalization ability of the evaluation are limited. Secondly, the F1 score only focuses on the complete match of components, instead of partial matches or semantic understanding. This may affect the actual performance of the parser. In contrast, using a standard corpus like Penn Treebank, is more reasonable for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Generate a grammar\n",
    "\n",
    "#### 4.1\n",
    "rubric= {accuracy:1}\n",
    "\n",
    "Parse trees implicitly contain the production rules for a CFG defined by the productions which are present in the parse tree. You can access these productions using the member function `nltk.Tree.productions()`. \n",
    "\n",
    "Produce a grammar corresponding to the CFG productions in your three sentences from exercise 2, and print it out. One \"trick\" to sort string in the manner you want is to create a sorting function that biases the sort by placing extra information (that will be sorted correctly) at the start of each string.  So you should write a sort function that does this, and pass it to the `sorted` function under the `key` parameter (similar to how we did lambda sorting in 521)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT -> S\n",
      "ROOT -> SINV\n",
      "S -> NP VP .\n",
      "S -> NP VP\n",
      "S -> INTJ , CC PP , NP VP .\n",
      "ADVP -> RB\n",
      "CC -> 'or'\n",
      "CC -> 'and'\n",
      "DT -> 'this'\n",
      "DT -> 'a'\n",
      "DT -> 'the'\n",
      "EX -> 'here'\n",
      "IN -> 'within'\n",
      "IN -> 'if'\n",
      "IN -> 'by'\n",
      "IN -> 'on'\n",
      "INTJ -> UH\n",
      "NML -> NP CC NP\n",
      "NML -> NNS CC NN NNS\n",
      "NN -> 'flick'\n",
      "NN -> 'body'\n",
      "NN -> 'movie'\n",
      "NN -> 'teen'\n",
      "NN -> 'slasher'\n",
      "NN -> 'power'\n",
      "NN -> 'way'\n",
      "NN -> 'horror'\n",
      "NNS -> 'parts'\n",
      "NNS -> 'robots'\n",
      "NP -> PRP\n",
      "NP -> DT\n",
      "NP -> NP PP\n",
      "NP -> EX\n",
      "NP -> NN NN\n",
      "NP -> DT NN\n",
      "NP -> NML\n",
      "NP -> RB\n",
      "NP -> PRP$ NN\n",
      "NP -> NML NN\n",
      "PP -> IN , S\n",
      "PP -> IN NP\n",
      "PP -> IN\n",
      "PRP -> 'they'\n",
      "PRP -> 'you'\n",
      "PRP$ -> 'your'\n",
      "RB -> 'really'\n",
      "RB -> 'little'\n",
      "RB -> 'not'\n",
      "RB -> 'so'\n",
      "SBAR -> S\n",
      "SINV -> ADVP , PP ADVP VP .\n",
      "UH -> 'oh'\n",
      "VBP -> 'do'\n",
      "VBP -> 'turn'\n",
      "VBP -> 'know'\n",
      "VBZ -> \"'s\"\n",
      "VBZ -> 'is'\n",
      "VP -> VBZ RB NP\n",
      "VP -> VBP NP\n",
      "VP -> VBP SBAR\n",
      "VP -> VBZ NP\n",
      ", -> ','\n",
      ". -> '.'\n"
     ]
    }
   ],
   "source": [
    "rules = set()\n",
    "\n",
    "# your code here\n",
    "\n",
    "# Sort the rules according to type: First ROOT rules, followed\n",
    "# by S rules, other non-terminals and finally terminal rules.\n",
    "# This is not a mandatory part of the assignment.\n",
    "\n",
    "for tree in gold_standard_parses:\n",
    "    rules.update(tree.productions())\n",
    "\n",
    "# Custom sort function for rules\n",
    "def sort_key(rule: nltk.grammar.Production) -> str:\n",
    "    lhs = str(rule.lhs())\n",
    "    if lhs == \"ROOT\":\n",
    "        return \"0\" + lhs\n",
    "    elif lhs == \"S\":\n",
    "        return \"1\" + lhs\n",
    "    elif lhs.isupper():\n",
    "        return \"2\" + lhs\n",
    "    else:\n",
    "        return \"3\" + lhs\n",
    "\n",
    "# Sort rules and print\n",
    "sorted_rules = sorted(rules, key=sort_key)\n",
    "for rule in sorted_rules:\n",
    "    print(rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2\n",
    "rubric= {accuracy:1}\n",
    "\n",
    "Show the rules in 4.1 are indeed sufficient to parse the sentences in the list `corpus`. Using an NLTK EarleyChartParser parser for this. Print out the number of parses for each sentence. Don't print out the parses themselves, as there might be a lot of them and you could crash your notebook (this depends a bit on how you fixed the parse trees in exercise 2). You should also set the `trace` keyword argument of the parser to 0 for the same reason. \n",
    "\n",
    "If you have individual sentences which are taking longer than 2 minutes to parse, you can skip over them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: 1 parses\n",
      "Sentence 2: 2 parses\n",
      "Sentence 3: 0 parses\n"
     ]
    }
   ],
   "source": [
    "from nltk import EarleyChartParser\n",
    "from nltk.grammar import CFG\n",
    "\n",
    "# your code here\n",
    "\n",
    "# Add terminal rules for all unique words in the corpus\n",
    "vocabulary = set()\n",
    "\n",
    "for sentence in corpus:\n",
    "    vocabulary.update(word.lower() for word in sentence)\n",
    "\n",
    "for word in vocabulary:\n",
    "    rules.add(Production(Nonterminal(word), [word]))\n",
    "\n",
    "# Create a CFG grammar from the rules\n",
    "grammar = CFG(Nonterminal('ROOT'), list(rules))\n",
    "\n",
    "# Initialize an EarleyChartParser with the grammar\n",
    "earley_parser = EarleyChartParser(grammar, trace=0)\n",
    "\n",
    "# Parse each sentence in the corpus\n",
    "for i, sentence in enumerate(corpus):\n",
    "    sentence = \" \".join(sentence)\n",
    "    parses = list(earley_parser.parse(sentence.split()))\n",
    "    print(f\"Sentence {i + 1}: {len(parses)} parses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3  Optional\n",
    "rubric= {accuracy:2}\n",
    "\n",
    "Convert your CFG grammar into a feature grammar and implement noun-verb agreement (you should use the feature values `3SG` and `NON3SG`). Make sure that all S, NP and VP rules use agreement features. \n",
    "\n",
    "Give an example sentence, which displays noun-verb agreement. Show that your feature grammar can parse this sentence. Then create a version of the same sentence without proper agreement, and show that the number of parses for this setence is lower (possibly zero). \n",
    "\n",
    "Your grammar shouldn't contain any rules where the LHS contains special characters like `.` or `$`. Otherwise `FeatureGrammar.fromstring` might give an error. This means that you might need to rename some of your non-terminals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}