
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>COLX 531 Lab 1. Machine Translation with Huggingface Transformers &#8212; Arno&#39;s Projects Collection</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'archive/COLX_531_Lab_1';</script>
    <link rel="canonical" href="docs.arnozeng.com/archive/COLX_531_Lab_1.html" />
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Coronary Heart Disease Prediction with Deep Learning" href="coronary-heart-disease-study.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Arno's Projects Collection - Home"/>
    <img src="../_static/logo.png" class="logo__image only-dark pst-js-only" alt="Arno's Projects Collection - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Arno’s Projects Collection
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="house-prices-py.html">House Prices - Advanced Regression Techniques (Python)</a></li>
<li class="toctree-l1"><a class="reference internal" href="house-prices-r.html">House Prices - Advanced Regression Techniques (R)</a></li>
<li class="toctree-l1"><a class="reference internal" href="twitter-sentiment-extactio.html">Tweet Sentiment Extraction</a></li>

<li class="toctree-l1"><a class="reference internal" href="coronary-heart-disease-study.html">Coronary Heart Disease Prediction with Deep Learning</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">COLX 531 Lab 1. Machine Translation with Huggingface Transformers</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/arnozeng98/data-science-archive" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/arnozeng98/data-science-archive/issues/new?title=Issue%20on%20page%20%2Farchive/COLX_531_Lab_1.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/archive/COLX_531_Lab_1.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>COLX 531 Lab 1. Machine Translation with Huggingface Transformers</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assignment-1-1">Assignment 1.1</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assignment-1-2">Assignment 1.2</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assignment-1-3">Assignment 1.3</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assignment-1-4">Assignment 1.4</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="colx-531-lab-1-machine-translation-with-huggingface-transformers">
<h1>COLX 531 Lab 1. Machine Translation with Huggingface Transformers<a class="headerlink" href="#colx-531-lab-1-machine-translation-with-huggingface-transformers" title="Link to this heading">#</a></h1>
<p>You can find the annotated version of this code <a class="reference external" href="https://huggingface.co/learn/nlp-course/en/chapter7/4">here</a>. Please make extensive use of the public documentation to understand what this notebook does.</p>
<p>Instructions:</p>
<ol class="arabic simple">
<li><p>Register for an account on <a class="reference external" href="https://huggingface.co/">HuggingFace</a>;</p></li>
<li><p>Register for an account on <a class="reference external" href="https://wandb.ai/site">Weights &amp; Biases</a>;</p></li>
<li><p>Run the training code below. You need to train two models. The first model is initialized randomly. The other model is initialized with pretrained weights. You are asked to log the whole training process with W&amp;B. Each run takes about 2 hours. Please do not change the code except for a few lines related to model initialization and logging.</p></li>
<li><p>When you submit, <strong>please also submit a pdf version of your notebook</strong>.</p></li>
</ol>
<section id="assignment-1-1">
<h2>Assignment 1.1<a class="headerlink" href="#assignment-1-1" title="Link to this heading">#</a></h2>
<p>rubric={accuracy:5}</p>
<p>After completing model training, push the trained model to HuggingFace hub under your own account. Please paste the link to your trained model below. The name of your model should be: <strong>your_username/lab1_finetuning</strong> and <strong>your_username/lab1_random</strong> for the randomly initialized model and the finetuned model respectively.</p>
<p><strong>Note</strong>. you must make the model public. Ohterwise we cannot grade your answer. If we cannot access your model page during grading, we will assign 0 to this question. In that case, a screenshot will be taken as evidence.</p>
<p><a class="reference external" href="https://huggingface.co/yiwenxxc/lab1_finetuning">https://huggingface.co/yiwenxxc/lab1_finetuning</a> \</p>
<p><a class="reference external" href="https://huggingface.co/yiwenxxc/lab1_random/settings">https://huggingface.co/yiwenxxc/lab1_random/settings</a>
<img alt="Q1.1.png" src="archive/attachment:e82b2ee4-4018-4e71-ad41-551e62cdf53f.png" />
<img alt="Q1.1_.png" src="archive/attachment:85adcdf2-f952-40b3-a615-17732efb798e.png" /></p>
</section>
<section id="assignment-1-2">
<h2>Assignment 1.2<a class="headerlink" href="#assignment-1-2" title="Link to this heading">#</a></h2>
<p>rubric={accuracy:5}</p>
<p>During model training, sync training statistics to your Weights &amp; Biases page under your own account. Please paste the link to your W&amp;B page to this model. You should use <strong>COLX531</strong> as the project name and <strong>lab1_random</strong> / <strong>lab1_finetuning</strong> as the run names for two models respectively.</p>
<p><strong>Note</strong>. you must make this W&amp;B page public <strong>and</strong> upload a screenshot of your W&amp;B page. Ohterwise we cannot grade your answer. If we cannot access your W&amp;B page during grading, we will assign 0 to this question. In that case, a screenshot will be taken as evidence.</p>
<p><a class="reference external" href="https://wandb.ai/tiffanychen020514-university-of-british-columbia/COLX531/runs/kfkb57vb/overview">https://wandb.ai/tiffanychen020514-university-of-british-columbia/COLX531/runs/kfkb57vb/overview</a> <br />
<a class="reference external" href="https://wandb.ai/tiffanychen020514-university-of-british-columbia/COLX531/runs/n5ddkkyk/overview">https://wandb.ai/tiffanychen020514-university-of-british-columbia/COLX531/runs/n5ddkkyk/overview</a>
<img alt="Q1.2.png" src="archive/attachment:22d83e43-3df3-4c7d-9d3e-4dcc95d05153.png" />
<img alt="Q1.2_.png" src="archive/attachment:e054fe41-0c20-452d-9fa0-558649a87d1b.png" /></p>
</section>
<section id="assignment-1-3">
<h2>Assignment 1.3<a class="headerlink" href="#assignment-1-3" title="Link to this heading">#</a></h2>
<p>rubric={accuracy:10}</p>
<p>Add a comment at <strong>each code block</strong> to explain what that block does. Comments must be <strong>your own words</strong>. Your comments should follow this format: # Comment: your explanations</p>
</section>
<section id="assignment-1-4">
<h2>Assignment 1.4<a class="headerlink" href="#assignment-1-4" title="Link to this heading">#</a></h2>
<p>rubric={accuracy:10}</p>
<p>Please answer the following questions <strong>in your own words</strong>. Ansers that are copied from existing sources without paraphrasing will not be graded.</p>
<ol class="arabic simple">
<li><p>What is <code class="docutils literal notranslate"><span class="pre">bleu</span></code> score? How is it calculated?</p></li>
</ol>
<ul class="simple">
<li><p>The BLEU score is a metric used to evaluate the quality of machine translation models by comparing generated translations with reference translations. It calculates how many words or phrases from the generated text appear in the reference translation. And the precision is served as the main measure. It is calculated using n-gram precision, applying a brevity penalty to avoid favoring short translations.</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p>What is the role of a data collator in this notebook?</p></li>
</ol>
<ul class="simple">
<li><p>It is responsible for grouping multiple training examples into batches and ensuring they are the same length. It does this by tokenizing the input and target texts and adding padding to shorter sequences so they match the longest sentence in the batch.</p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p>How much memory on average does evaluating a MT model cost in this notebook? (Inspect your W&amp;B page).
-<img alt="Q1.4_1.png" src="archive/attachment:cbcc64cd-f214-4b5d-886a-94c69f91faf1.png" /></p></li>
<li><p>How much memory on average does training a MT model cost in this notebook? (Inspect your W&amp;B page).
-<img alt="Q1.4_2.png" src="archive/attachment:397d48f1-6691-4c14-b245-9703bbd5b299.png" /></p></li>
<li><p>Describe the differences (if any) in terms of loss, evaluation results, training speed and memory usage between a randomly initialized model and a finetuned model initialized with pretrained weights. (Inspect your W&amp;B page).</p></li>
</ol>
<ul class="simple">
<li><p>The fine-tuned model has a much lower training loss of 1.12 and evaluation loss of 1.03, while the randomly initialized model has significantly higher losses, with 5.33 for training and 5.25 for evaluation. This shows that the pretrained model learns faster and reaches better performance in fewer steps. The BLEU score for the fine-tuned model is 49.2, meaning its translations are much closer to the reference texts, while the randomly initialized model only achieves 6.52, indicating poor translation quality. In terms of training speed, the fine-tuned model processes more samples per second compared to the randomly initialized one, and it also completes more training steps per second. This suggests that the pretrained model is more efficient since it starts with existing knowledge rather than learning everything from scratch. Memory usage remains similar between both models because they share the same architecture and batch size.</p></li>
</ul>
<p>Install the wandb, Transformers, Datasets, and Evaluate libraries to run this notebook.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install the wandb, Transformers, Datasets, and Evaluate libraries  (your comment should look like this)</span>
<span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>datasets<span class="w"> </span>evaluate<span class="w"> </span>transformers<span class="o">[</span>sentencepiece<span class="o">]</span>
<span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>accelerate
<span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>wandb
<span class="c1"># To run the training on TPU, you will need to uncomment the following line:</span>
<span class="c1"># !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl</span>
<span class="o">!</span>apt<span class="w"> </span>install<span class="w"> </span>git-lfs
</pre></div>
</div>
</div>
</div>
<p>You will need to setup git, adapt your email and name in the following cell.</p>
<p>You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># log in to your own HuggingFace account</span>
<span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">notebook_login</span>
<span class="n">notebook_login</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "717ba9d3f1d9450bb037b3f205678d96", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># log in to your own wandb account</span>
<span class="kn">import</span> <span class="nn">wandb</span>
<span class="n">wandb</span><span class="o">.</span><span class="n">login</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="s1">&#39;ab40927d7ff45aebac010a97159be0d887c9feb2&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold -Color-Bold-Blue">wandb</span>: <span class=" -Color -Color-Yellow">WARNING</span> If you&#39;re specifying your api key in code, ensure this code is not shared publicly.
<span class=" -Color -Color-Bold -Color-Bold-Blue">wandb</span>: <span class=" -Color -Color-Yellow">WARNING</span> Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.
<span class=" -Color -Color-Bold -Color-Bold-Blue">wandb</span>: Appending key for api.wandb.ai to your netrc file: /root/.netrc
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="n">raw_datasets</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;kde4&quot;</span><span class="p">,</span> <span class="n">lang1</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">lang2</span><span class="o">=</span><span class="s2">&quot;fr&quot;</span><span class="p">)</span> <span class="c1"># kde4 is dataset identifier, en =  English, fr = French</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">raw_datasets</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>DatasetDict({
    train: Dataset({
        features: [&#39;id&#39;, &#39;translation&#39;],
        num_rows: 210173
    })
})
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">split_datasets</span> <span class="o">=</span> <span class="n">raw_datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">train_size</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span> <span class="c1">#Split the training dataset into two parts, 90% for training and 10% for validation. Seed is set to 20 to make sure the split is same every time we run the code.</span>
<span class="n">split_datasets</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>DatasetDict({
    train: Dataset({
        features: [&#39;id&#39;, &#39;translation&#39;],
        num_rows: 189155
    })
    test: Dataset({
        features: [&#39;id&#39;, &#39;translation&#39;],
        num_rows: 21018
    })
})
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">split_datasets</span><span class="p">[</span><span class="s2">&quot;validation&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">split_datasets</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;test&quot;</span><span class="p">)</span> <span class="c1"># Rename the &quot;test&quot; split to &quot;validation&quot;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">split_datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;translation&quot;</span><span class="p">]</span> <span class="c1">#Get the second example from the training dataset and retrieve its &quot;translation&quot;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;en&#39;: &#39;Default to expanded threads&#39;,
 &#39;fr&#39;: &#39;Par défaut, développer les fils de discussion&#39;}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="n">model_checkpoint</span> <span class="o">=</span> <span class="s2">&quot;Helsinki-NLP/opus-mt-en-fr&quot;</span> <span class="c1"># Define the model checkpoint which is for English-to-French translation model from Helsinki-NLP.</span>
<span class="n">translator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;translation&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_checkpoint</span><span class="p">)</span> <span class="c1"># Create a translation pipeline using the model checkpoint defined above.</span>
<span class="n">translator</span><span class="p">(</span><span class="s2">&quot;Default to expanded threads&quot;</span><span class="p">)</span> <span class="c1"># Translate given sentence into French</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Device set to use cuda:0
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[{&#39;translation_text&#39;: &#39;Par défaut pour les threads élargis&#39;}]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">split_datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="mi">172</span><span class="p">][</span><span class="s2">&quot;translation&quot;</span><span class="p">]</span> <span class="c1"># #Get the 173rd example from the training dataset and retrieve its &quot;translation&quot;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;en&#39;: &#39;Unable to import %1 using the OFX importer plugin. This file is not the correct format.&#39;,
 &#39;fr&#39;: &quot;Impossible d&#39;importer %1 en utilisant le module d&#39;extension d&#39;importation OFX. Ce fichier n&#39;a pas un format correct.&quot;}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">translator</span><span class="p">(</span>
    <span class="s2">&quot;Unable to import %1 using the OFX importer plugin. This file is not the correct format.&quot;</span>
<span class="p">)</span>
<span class="c1"># # Translate given sentence into French</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[{&#39;translation_text&#39;: &quot;Impossible d&#39;importer %1 en utilisant le plugin d&#39;importateur OFX. Ce fichier n&#39;est pas le bon format.&quot;}]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">model_checkpoint</span> <span class="o">=</span> <span class="s2">&quot;Helsinki-NLP/opus-mt-en-fr&quot;</span> <span class="c1"># Define the model checkpoint which is for English-to-French translation model from Helsinki-NLP.</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_checkpoint</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span> <span class="c1"># Load the tokenizer using the model checkpoint defined above. The &#39;return_tensors=&quot;pt&quot;&#39; argument ensures the tokens are returned as PyTorch tensors.</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">en_sentence</span> <span class="o">=</span> <span class="n">split_datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;translation&quot;</span><span class="p">][</span><span class="s2">&quot;en&quot;</span><span class="p">]</span> <span class="c1"># Get english translation of second example from the training dataset</span>
<span class="n">fr_sentence</span> <span class="o">=</span> <span class="n">split_datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;translation&quot;</span><span class="p">][</span><span class="s2">&quot;fr&quot;</span><span class="p">]</span> <span class="c1"># Get french translation of second example from the training dataset</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">en_sentence</span><span class="p">,</span> <span class="n">text_target</span><span class="o">=</span><span class="n">fr_sentence</span><span class="p">)</span> <span class="c1"># Tokenize both the English input sentence and the French target sentence, &#39;text_target&#39; specifies the reference translation</span>
<span class="n">inputs</span> <span class="c1"># show tokenized output</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;input_ids&#39;: [47591, 12, 9842, 19634, 9, 0], &#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1], &#39;labels&#39;: [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wrong_targets</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">fr_sentence</span><span class="p">)</span> <span class="c1"># Tokenize only the French sentence without specifying it as a target</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">wrong_targets</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]))</span> <span class="c1"># Convert the tokenized input IDs into their corresponding tokens and print them.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]))</span> <span class="c1"># Convert the correctly tokenized target labels into tokens and print them.</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">max_length</span> <span class="o">=</span> <span class="mi">128</span> <span class="c1"># Define the maximum sequence length for tokenization is 128</span>


<span class="k">def</span> <span class="nf">preprocess_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span> <span class="c1"># Define a function to preprocess a batch of examples for training.</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">ex</span><span class="p">[</span><span class="s2">&quot;en&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">ex</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">[</span><span class="s2">&quot;translation&quot;</span><span class="p">]]</span> <span class="c1"># Extract the English sentences from &quot;en&quot; field inside the &quot;translation&quot; dictionary as inputs.</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="p">[</span><span class="n">ex</span><span class="p">[</span><span class="s2">&quot;fr&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">ex</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">[</span><span class="s2">&quot;translation&quot;</span><span class="p">]]</span> <span class="c1"># Extract the French sentences from &quot;fr&quot; field inside the &quot;translation&quot; dictionary as targets.</span>
    <span class="c1"># tokenize both English inputs and French targets.</span>
    <span class="n">model_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">text_target</span><span class="o">=</span><span class="n">targets</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">model_inputs</span> <span class="c1"># Return the tokenized inputs and target sequences</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokenized_datasets</span> <span class="o">=</span> <span class="n">split_datasets</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
    <span class="n">preprocess_function</span><span class="p">,</span>
    <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">remove_columns</span><span class="o">=</span><span class="n">split_datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">column_names</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># This code apply preprocessing function to dataset to tokenize input and target text. batched=True means it process multiple example at same time for better speed. Original columns removed because after tokenization we only need token ids for training.</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSeq2SeqLM</span> <span class="c1"># AutoModelForSeq2SeqLM is used for sequence-to-sequence tasks like translation.</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_checkpoint</span><span class="p">)</span> <span class="c1"># Load the pretrained model from the checkpoint, so it already has knowledge from previous training.</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DataCollatorForSeq2Seq</span> <span class="c1"># DataCollatorForSeq2Seq helps in batching sequences of different lengths.</span>

<span class="n">data_collator</span> <span class="o">=</span> <span class="n">DataCollatorForSeq2Seq</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">)</span> <span class="c1"># Create a data collator that ensures all sequences in a batch have the same length.</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">batch</span> <span class="o">=</span> <span class="n">data_collator</span><span class="p">([</span><span class="n">tokenized_datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)])</span> <span class="c1"># Create a batch of tokenized examples from index 1 to 2</span>
<span class="n">batch</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>dict_keys([&#39;input_ids&#39;, &#39;attention_mask&#39;, &#39;labels&#39;, &#39;decoder_input_ids&#39;])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span> <span class="c1"># show labels from batch, which are tokenized French sentences.</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[  577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,  -100,
          -100,  -100,  -100,  -100,  -100,  -100],
        [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,   817,
           550,  7032,  5821,  7907, 12649,     0]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">]</span> <span class="c1"># show decoder input IDs from batch</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[59513,   577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,
         59513, 59513, 59513, 59513, 59513, 59513],
        [59513,  1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,
           817,   550,  7032,  5821,  7907, 12649]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">tokenized_datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;labels&quot;</span><span class="p">])</span> <span class="c1"># Loop and print tokenized target sentences.</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>sacrebleu<span class="w"> </span>#<span class="w"> </span>Install<span class="w"> </span>the<span class="w"> </span>sacrebleu<span class="w"> </span>library<span class="w"> </span>to<span class="w"> </span>evaluate<span class="w"> </span>machine<span class="w"> </span>translation<span class="w"> </span>quality
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">evaluate</span>

<span class="n">metric</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;sacrebleu&quot;</span><span class="p">)</span> <span class="c1"># Load the evaluation metric</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a list of predicted translations</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;This plugin lets you translate web pages between several languages automatically.&quot;</span>
<span class="p">]</span>
<span class="n">references</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span>
        <span class="s2">&quot;This plugin allows you to automatically translate web pages between several languages.&quot;</span>
    <span class="p">]</span>
<span class="p">]</span> <span class="c1"># Define a list of reference translations</span>
<span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">references</span><span class="p">)</span> <span class="c1"># compute the BLEU score using the sacrebleu metric</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;score&#39;: 46.750469682990165,
 &#39;counts&#39;: [11, 6, 4, 3],
 &#39;totals&#39;: [12, 11, 10, 9],
 &#39;precisions&#39;: [91.66666666666667,
  54.54545454545455,
  40.0,
  33.333333333333336],
 &#39;bp&#39;: 0.9200444146293233,
 &#39;sys_len&#39;: 12,
 &#39;ref_len&#39;: 13}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictions</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;This This This This&quot;</span><span class="p">]</span> <span class="c1"># define an incorrect repetition of this case</span>
<span class="n">references</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span>
        <span class="s2">&quot;This plugin allows you to automatically translate web pages between several languages.&quot;</span>
    <span class="p">]</span>
<span class="p">]</span> <span class="c1"># Define a list of reference translations</span>
<span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">references</span><span class="p">)</span><span class="c1"># compute the BLEU score using the sacrebleu metric</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;score&#39;: 1.683602693167689,
 &#39;counts&#39;: [1, 0, 0, 0],
 &#39;totals&#39;: [4, 3, 2, 1],
 &#39;precisions&#39;: [25.0, 16.666666666666668, 12.5, 12.5],
 &#39;bp&#39;: 0.10539922456186433,
 &#39;sys_len&#39;: 4,
 &#39;ref_len&#39;: 13}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictions</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;This plugin&quot;</span><span class="p">]</span> <span class="c1"># define an imcomplete phrase</span>
<span class="n">references</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span>
        <span class="s2">&quot;This plugin allows you to automatically translate web pages between several languages.&quot;</span>
    <span class="p">]</span>
<span class="p">]</span><span class="c1"># Define a list of reference translations</span>
<span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">references</span><span class="p">)</span><span class="c1"># compute the BLEU score using the sacrebleu metric</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;score&#39;: 0.0,
 &#39;counts&#39;: [2, 1, 0, 0],
 &#39;totals&#39;: [2, 1, 0, 0],
 &#39;precisions&#39;: [100.0, 100.0, 0.0, 0.0],
 &#39;bp&#39;: 0.004086771438464067,
 &#39;sys_len&#39;: 2,
 &#39;ref_len&#39;: 13}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_preds</span><span class="p">):</span>
    <span class="n">preds</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_preds</span> <span class="c1"># Unpack predictions and labels</span>
    <span class="c1"># In case the model returns more than the prediction logits</span>
    <span class="c1"># If the model returns multiple outputs, extract only the main prediction logits</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="n">preds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># Convert tokenized predictions back into readable text, ignoring special tokens</span>
    <span class="n">decoded_preds</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Replace -100s in the labels as we can&#39;t decode them</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">labels</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">100</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span>
    <span class="n">decoded_labels</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Some simple post-processing</span>
    <span class="n">decoded_preds</span> <span class="o">=</span> <span class="p">[</span><span class="n">pred</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">pred</span> <span class="ow">in</span> <span class="n">decoded_preds</span><span class="p">]</span>
    <span class="n">decoded_labels</span> <span class="o">=</span> <span class="p">[[</span><span class="n">label</span><span class="o">.</span><span class="n">strip</span><span class="p">()]</span> <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">decoded_labels</span><span class="p">]</span>
    <span class="c1"># Compute BLEU score</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">decoded_preds</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">decoded_labels</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;bleu&quot;</span><span class="p">:</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;score&quot;</span><span class="p">]}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Seq2SeqTrainingArguments</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="c1"># specifying the project name as &quot;COLX531&quot; in wb</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;WANDB_PROJECT&quot;</span><span class="p">]</span><span class="o">=</span><span class="s2">&quot;COLX531&quot;</span>

<span class="n">args</span> <span class="o">=</span> <span class="n">Seq2SeqTrainingArguments</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;lab1_finetuning&quot;</span><span class="p">,</span> <span class="c1"># output directory name</span>
    <span class="n">evaluation_strategy</span><span class="o">=</span><span class="s2">&quot;no&quot;</span><span class="p">,</span> <span class="c1"># Disable evaluation</span>
    <span class="n">save_strategy</span><span class="o">=</span><span class="s2">&quot;steps&quot;</span><span class="p">,</span> <span class="c1"># Save model checkpoints based on steps</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-5</span><span class="p">,</span> <span class="c1"># Set the learning rate</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="c1"># Define batch size for training of 16</span>
    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="c1"># Define batch size for evaluation of 32</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="c1"># it is used to reduce overfitting and improve generalization</span>
    <span class="n">save_total_limit</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="c1"># Limit the number of saved checkpoints to 2</span>
    <span class="n">max_steps</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="c1"># if you are running ths on a GPU, you can change it to 5000 for better performance.</span>
    <span class="n">predict_with_generate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># Enable generation-based prediction</span>
    <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># Enable faster computation and reduced memory usage</span>
    <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># Push the trained model to the Hugging Face automatically</span>
    <span class="n">report_to</span><span class="o">=</span><span class="s2">&quot;wandb&quot;</span><span class="p">,</span>  <span class="c1"># turn on W&amp;B logging</span>
    <span class="n">run_name</span><span class="o">=</span><span class="s2">&quot;lab1_finetuning&quot;</span><span class="p">,</span> <span class="c1"># change model name</span>
    <span class="n">logging_steps</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="c1"># Log training metrics every 500 steps to monitor progress</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Seq2SeqTrainer</span>
<span class="c1"># Create a Seq2SeqTrainer trainer including training, evaluation, and logging automatically</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Seq2SeqTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">tokenized_datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">],</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">tokenized_datasets</span><span class="p">[</span><span class="s2">&quot;validation&quot;</span><span class="p">],</span>
    <span class="n">data_collator</span><span class="o">=</span><span class="n">data_collator</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;ipython-input-63-2b868b8851f1&gt;:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">)</span> <span class="c1"># Evaluate the model on the validation dataset</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
    <div>
      
      <progress value='493' max='657' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [493/657 17:50 < 05:56, 0.46 it/s]
    </div>
    </div><div class="output text_html">
    <div>
      
      <progress value='657' max='657' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [657/657 23:28]
    </div>
    </div><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold -Color-Bold-Blue">wandb</span>: <span class=" -Color -Color-Yellow">WARNING</span> The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
</pre></div>
</div>
<div class="output text_html">Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target="_blank">the W&B docs</a>.</div><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold -Color-Bold-Blue">wandb</span>: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
</pre></div>
</div>
<div class="output text_html">Tracking run with wandb version 0.19.6</div><div class="output text_html">Run data is saved locally in <code>/content/wandb/run-20250219_010853-kfkb57vb</code></div><div class="output text_html">Syncing run <strong><a href='https://wandb.ai/tiffanychen020514-university-of-british-columbia/COLX531/runs/kfkb57vb' target="_blank">lab1_finetuning</a></strong> to <a href='https://wandb.ai/tiffanychen020514-university-of-british-columbia/COLX531' target="_blank">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target="_blank">docs</a>)<br></div><div class="output text_html"> View project at <a href='https://wandb.ai/tiffanychen020514-university-of-british-columbia/COLX531' target="_blank">https://wandb.ai/tiffanychen020514-university-of-british-columbia/COLX531</a></div><div class="output text_html"> View run at <a href='https://wandb.ai/tiffanychen020514-university-of-british-columbia/COLX531/runs/kfkb57vb' target="_blank">https://wandb.ai/tiffanychen020514-university-of-british-columbia/COLX531/runs/kfkb57vb</a></div><div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;eval_loss&#39;: 1.7061808109283447,
 &#39;eval_model_preparation_time&#39;: 0.0054,
 &#39;eval_bleu&#39;: 39.268924405769546,
 &#39;eval_runtime&#39;: 1509.1007,
 &#39;eval_samples_per_second&#39;: 13.927,
 &#39;eval_steps_per_second&#39;: 0.435}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span> <span class="c1"># Start the fine-tuning process</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
    <div>
      
      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [5000/5000 11:08, Epoch 0/1]
    </div>
    <table border="1" class="dataframe">
  <thead>
 <tr style="text-align: left;">
      <th>Step</th>
      <th>Training Loss</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>500</td>
      <td>1.435500</td>
    </tr>
    <tr>
      <td>1000</td>
      <td>1.297500</td>
    </tr>
    <tr>
      <td>1500</td>
      <td>1.246600</td>
    </tr>
    <tr>
      <td>2000</td>
      <td>1.190600</td>
    </tr>
    <tr>
      <td>2500</td>
      <td>1.198300</td>
    </tr>
    <tr>
      <td>3000</td>
      <td>1.187900</td>
    </tr>
    <tr>
      <td>3500</td>
      <td>1.148600</td>
    </tr>
    <tr>
      <td>4000</td>
      <td>1.147000</td>
    </tr>
    <tr>
      <td>4500</td>
      <td>1.135700</td>
    </tr>
    <tr>
      <td>5000</td>
      <td>1.129600</td>
    </tr>
  </tbody>
</table><p></div><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:2758: UserWarning: Moving the following attributes in the config to the generation config: {&#39;max_length&#39;: 512, &#39;num_beams&#39;: 4, &#39;bad_words_ids&#39;: [[59513]]}. You are seeing this warning because you&#39;ve set generation parameters in the model config, as opposed to in the generation config.
  warnings.warn(
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TrainOutput(global_step=5000, training_loss=1.2117285766601562, metrics={&#39;train_runtime&#39;: 669.2214, &#39;train_samples_per_second&#39;: 119.542, &#39;train_steps_per_second&#39;: 7.471, &#39;total_flos&#39;: 1191976680554496.0, &#39;train_loss&#39;: 1.2117285766601562, &#39;epoch&#39;: 0.422904508162057})
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">)</span> <span class="c1"># Evaluate the model on the validation dataset</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
    <div>
      
      <progress value='1752' max='657' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [657/657 1:21:14]
    </div>
    </div><div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;eval_loss&#39;: 1.0384902954101562,
 &#39;eval_model_preparation_time&#39;: 0.0054,
 &#39;eval_bleu&#39;: 49.27394398861422,
 &#39;eval_runtime&#39;: 1672.952,
 &#39;eval_samples_per_second&#39;: 12.563,
 &#39;eval_steps_per_second&#39;: 0.393,
 &#39;epoch&#39;: 0.422904508162057}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># modify this code to push trained model to your own HF account</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="s1">&#39;lab1_finetuning&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>No files have been modified since last commit. Skipping to prevent empty commit.
WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CommitInfo(commit_url=&#39;https://huggingface.co/yiwenxxc/lab1_finetuning/commit/bd6f623ef8fb67e949b682cdafed4a6f61b5d669&#39;, commit_message=&#39;lab1_finetuning&#39;, commit_description=&#39;&#39;, oid=&#39;bd6f623ef8fb67e949b682cdafed4a6f61b5d669&#39;, pr_url=None, repo_url=RepoUrl(&#39;https://huggingface.co/yiwenxxc/lab1_finetuning&#39;, endpoint=&#39;https://huggingface.co&#39;, repo_type=&#39;model&#39;, repo_id=&#39;yiwenxxc/lab1_finetuning&#39;), pr_revision=None, pr_num=None)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wandb</span><span class="o">.</span><span class="n">finish</span><span class="p">()</span>  <span class="c1"># Finish the current run</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"></div><div class="output text_html"><br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class="wandb-row"><div class="wandb-col"><h3>Run history:</h3><br/><table class="wandb"><tr><td>eval/bleu</td><td>▁█</td></tr><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/model_preparation_time</td><td>▁▁</td></tr><tr><td>eval/runtime</td><td>▁█</td></tr><tr><td>eval/samples_per_second</td><td>█▁</td></tr><tr><td>eval/steps_per_second</td><td>█▁</td></tr><tr><td>train/epoch</td><td>▁▂▃▃▄▅▆▆▇███</td></tr><tr><td>train/global_step</td><td>▁▂▂▃▄▅▅▆▇▇███</td></tr><tr><td>train/grad_norm</td><td>█▆▃▂▅▇▃▁▂▃</td></tr><tr><td>train/learning_rate</td><td>█▇▆▆▅▄▃▃▂▁</td></tr><tr><td>train/loss</td><td>█▅▄▂▃▂▁▁▁▁</td></tr></table><br/></div><div class="wandb-col"><h3>Run summary:</h3><br/><table class="wandb"><tr><td>eval/bleu</td><td>49.27394</td></tr><tr><td>eval/loss</td><td>1.03849</td></tr><tr><td>eval/model_preparation_time</td><td>0.0054</td></tr><tr><td>eval/runtime</td><td>1672.952</td></tr><tr><td>eval/samples_per_second</td><td>12.563</td></tr><tr><td>eval/steps_per_second</td><td>0.393</td></tr><tr><td>total_flos</td><td>1191976680554496.0</td></tr><tr><td>train/epoch</td><td>0.4229</td></tr><tr><td>train/global_step</td><td>5000</td></tr><tr><td>train/grad_norm</td><td>6.25913</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.1296</td></tr><tr><td>train_loss</td><td>1.21173</td></tr><tr><td>train_runtime</td><td>669.2214</td></tr><tr><td>train_samples_per_second</td><td>119.542</td></tr><tr><td>train_steps_per_second</td><td>7.471</td></tr></table><br/></div></div></div><div class="output text_html"> View run <strong style="color:#cdcd00">lab1_finetuning</strong> at: <a href='https://wandb.ai/tiffanychen020514-university-of-british-columbia/COLX531/runs/kfkb57vb' target="_blank">https://wandb.ai/tiffanychen020514-university-of-british-columbia/COLX531/runs/kfkb57vb</a><br> View project at: <a href='https://wandb.ai/tiffanychen020514-university-of-british-columbia/COLX531' target="_blank">https://wandb.ai/tiffanychen020514-university-of-british-columbia/COLX531</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)</div><div class="output text_html">Find logs at: <code>./wandb/run-20250219_010853-kfkb57vb/logs</code></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wandb</span><span class="o">.</span><span class="n">login</span><span class="p">()</span> <span class="c1"># Logging in again</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_checkpoint</span><span class="p">)</span>
<span class="n">random_model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

<span class="n">args_random</span> <span class="o">=</span> <span class="n">Seq2SeqTrainingArguments</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;lab1_random&quot;</span><span class="p">,</span> <span class="c1"># output directory name</span>
    <span class="n">evaluation_strategy</span><span class="o">=</span><span class="s2">&quot;no&quot;</span><span class="p">,</span> <span class="c1"># Disable evaluation</span>
    <span class="n">save_strategy</span><span class="o">=</span><span class="s2">&quot;steps&quot;</span><span class="p">,</span> <span class="c1"># Save model checkpoints based on steps</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-5</span><span class="p">,</span> <span class="c1"># Set the learning rate</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="c1"># Define batch size for training of 16</span>
    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="c1"># Define batch size for evaluation of 32</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="c1"># it is used to reduce overfitting and improve generalization</span>
    <span class="n">save_total_limit</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="c1"># Limit the number of saved checkpoints to 2</span>
    <span class="n">max_steps</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="c1"># if you are running ths on a GPU, you can change it to 5000 for better performance.</span>
    <span class="n">predict_with_generate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># Enable generation-based prediction</span>
    <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># Enable faster computation and reduced memory usage</span>
    <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># Push the trained model to the Hugging Face automatically</span>
    <span class="n">report_to</span><span class="o">=</span><span class="s2">&quot;wandb&quot;</span><span class="p">,</span>  <span class="c1"># turn on W&amp;B logging</span>
    <span class="n">run_name</span><span class="o">=</span><span class="s2">&quot;lab1_random&quot;</span><span class="p">,</span> <span class="c1"># change model name</span>
    <span class="n">logging_steps</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="c1"># Log training metrics every 500 steps to monitor progress</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a Seq2SeqTrainer trainer including training, evaluation, and logging automatically</span>
<span class="n">trainer_random</span> <span class="o">=</span> <span class="n">Seq2SeqTrainer</span><span class="p">(</span>
    <span class="n">random_model</span><span class="p">,</span>
    <span class="n">args_random</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">tokenized_datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">],</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">tokenized_datasets</span><span class="p">[</span><span class="s2">&quot;validation&quot;</span><span class="p">],</span>
    <span class="n">data_collator</span><span class="o">=</span><span class="n">data_collator</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;ipython-input-78-dbeaa67f4b0c&gt;:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer_random = Seq2SeqTrainer(
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trainer_random</span><span class="o">.</span><span class="n">train</span><span class="p">()</span> <span class="c1"># train the model</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target="_blank">the W&B docs</a>.</div><div class="output text_html">Tracking run with wandb version 0.19.6</div><div class="output text_html">Run data is saved locally in <code>/content/wandb/run-20250219_022554-n5ddkkyk</code></div><div class="output text_html">Syncing run <strong><a href='https://wandb.ai/tiffanychen020514-university-of-british-columbia/COLX531/runs/n5ddkkyk' target="_blank">lab1_random</a></strong> to <a href='https://wandb.ai/tiffanychen020514-university-of-british-columbia/COLX531' target="_blank">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target="_blank">docs</a>)<br></div><div class="output text_html"> View project at <a href='https://wandb.ai/tiffanychen020514-university-of-british-columbia/COLX531' target="_blank">https://wandb.ai/tiffanychen020514-university-of-british-columbia/COLX531</a></div><div class="output text_html"> View run at <a href='https://wandb.ai/tiffanychen020514-university-of-british-columbia/COLX531/runs/n5ddkkyk' target="_blank">https://wandb.ai/tiffanychen020514-university-of-british-columbia/COLX531/runs/n5ddkkyk</a></div><div class="output text_html">
    <div>
      
      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [5000/5000 11:41, Epoch 0/1]
    </div>
    <table border="1" class="dataframe">
  <thead>
 <tr style="text-align: left;">
      <th>Step</th>
      <th>Training Loss</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>500</td>
      <td>8.029900</td>
    </tr>
    <tr>
      <td>1000</td>
      <td>6.469800</td>
    </tr>
    <tr>
      <td>1500</td>
      <td>6.098500</td>
    </tr>
    <tr>
      <td>2000</td>
      <td>5.837900</td>
    </tr>
    <tr>
      <td>2500</td>
      <td>5.670000</td>
    </tr>
    <tr>
      <td>3000</td>
      <td>5.550200</td>
    </tr>
    <tr>
      <td>3500</td>
      <td>5.439800</td>
    </tr>
    <tr>
      <td>4000</td>
      <td>5.414400</td>
    </tr>
    <tr>
      <td>4500</td>
      <td>5.358000</td>
    </tr>
    <tr>
      <td>5000</td>
      <td>5.332500</td>
    </tr>
  </tbody>
</table><p></div><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:2758: UserWarning: Moving the following attributes in the config to the generation config: {&#39;max_length&#39;: 512, &#39;num_beams&#39;: 4, &#39;bad_words_ids&#39;: [[59513]]}. You are seeing this warning because you&#39;ve set generation parameters in the model config, as opposed to in the generation config.
  warnings.warn(
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TrainOutput(global_step=5000, training_loss=5.920086328125, metrics={&#39;train_runtime&#39;: 702.1643, &#39;train_samples_per_second&#39;: 113.933, &#39;train_steps_per_second&#39;: 7.121, &#39;total_flos&#39;: 1191976680554496.0, &#39;train_loss&#39;: 5.920086328125, &#39;epoch&#39;: 0.422904508162057})
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trainer_random</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">)</span> <span class="c1"># Evaluate the random model on the validation dataset</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
    <div>
      
      <progress value='657' max='657' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [657/657 32:00]
    </div>
    </div><div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;eval_loss&#39;: 5.256569862365723,
 &#39;eval_bleu&#39;: 6.528653955380906,
 &#39;eval_runtime&#39;: 2038.2056,
 &#39;eval_samples_per_second&#39;: 10.312,
 &#39;eval_steps_per_second&#39;: 0.322,
 &#39;epoch&#39;: 0.422904508162057}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trainer_random</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="s2">&quot;lab1_random&quot;</span><span class="p">)</span> <span class="c1"># push to hugging face</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CommitInfo(commit_url=&#39;https://huggingface.co/yiwenxxc/lab1_random/commit/c39467d3614001f9b68489fdf5794950eb92fb09&#39;, commit_message=&#39;lab1_random&#39;, commit_description=&#39;&#39;, oid=&#39;c39467d3614001f9b68489fdf5794950eb92fb09&#39;, pr_url=None, repo_url=RepoUrl(&#39;https://huggingface.co/yiwenxxc/lab1_random&#39;, endpoint=&#39;https://huggingface.co&#39;, repo_type=&#39;model&#39;, repo_id=&#39;yiwenxxc/lab1_random&#39;), pr_revision=None, pr_num=None)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wandb</span><span class="o">.</span><span class="n">finish</span><span class="p">()</span>  <span class="c1"># Finish the current run</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"></div><div class="output text_html"><br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class="wandb-row"><div class="wandb-col"><h3>Run history:</h3><br/><table class="wandb"><tr><td>eval/bleu</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▂▃▃▄▅▆▆▇███</td></tr><tr><td>train/global_step</td><td>▁▂▃▃▄▅▆▆▇███</td></tr><tr><td>train/grad_norm</td><td>▁▅▁▁▄█▃▂▃▆</td></tr><tr><td>train/learning_rate</td><td>█▇▆▆▅▄▃▃▂▁</td></tr><tr><td>train/loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr></table><br/></div><div class="wandb-col"><h3>Run summary:</h3><br/><table class="wandb"><tr><td>eval/bleu</td><td>6.52865</td></tr><tr><td>eval/loss</td><td>5.25657</td></tr><tr><td>eval/runtime</td><td>2038.2056</td></tr><tr><td>eval/samples_per_second</td><td>10.312</td></tr><tr><td>eval/steps_per_second</td><td>0.322</td></tr><tr><td>total_flos</td><td>1191976680554496.0</td></tr><tr><td>train/epoch</td><td>0.4229</td></tr><tr><td>train/global_step</td><td>5000</td></tr><tr><td>train/grad_norm</td><td>2.97382</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>5.3325</td></tr><tr><td>train_loss</td><td>5.92009</td></tr><tr><td>train_runtime</td><td>702.1643</td></tr><tr><td>train_samples_per_second</td><td>113.933</td></tr><tr><td>train_steps_per_second</td><td>7.121</td></tr></table><br/></div></div></div><div class="output text_html"> View run <strong style="color:#cdcd00">lab1_random</strong> at: <a href='https://wandb.ai/tiffanychen020514-university-of-british-columbia/COLX531/runs/n5ddkkyk' target="_blank">https://wandb.ai/tiffanychen020514-university-of-british-columbia/COLX531/runs/n5ddkkyk</a><br> View project at: <a href='https://wandb.ai/tiffanychen020514-university-of-british-columbia/COLX531' target="_blank">https://wandb.ai/tiffanychen020514-university-of-british-columbia/COLX531</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)</div><div class="output text_html">Find logs at: <code>./wandb/run-20250219_022554-n5ddkkyk/logs</code></div></div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "conda-base-py"
        },
        kernelOptions: {
            name: "conda-base-py",
            path: "./archive"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'conda-base-py'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="coronary-heart-disease-study.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Coronary Heart Disease Prediction with Deep Learning</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assignment-1-1">Assignment 1.1</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assignment-1-2">Assignment 1.2</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assignment-1-3">Assignment 1.3</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assignment-1-4">Assignment 1.4</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Arno Zeng
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>